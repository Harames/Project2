logForestAge   <- log(ForestAge + .5)
myMatrix       <- cbind(PctCover,logForestAge,logSalamanders)
if (require(car)) { # Use car library
scatterplotMatrix(myMatrix, diagonal="histogram", reg.line=FALSE, spread=FALSE)
}
model1 = lm(log(Salamanders+1)~PctCover+logForestAge)
summary(model1)
plot(model1)
myGlm1  <- glm(Salamanders ~ PctCover + logForestAge + PctCover:logForestAge,
family=poisson)
summary(myGlm1)   # Backward elimination...
myGlm2 <- update(myGlm1, ~ . - PctCover:logForestAge)
summary(myGlm2)
myGlm3  <- update(myGlm2, ~ . - logForestAge)
summary(myGlm3)   # PctCover is the only explanatory variable remaining
plot(Salamanders ~ PctCover)  # It appears that there are 2 distributions
df1 <- read.csv("day1.csv")
df1 <- read.csv("day1.csv")
df1 <- read.csv("day1.csv")
df1 <- read.csv("day1.csv")
df2 = read.csv("day2.csv")
df3 <- read.csv("day3.csv")
df = rbind(df1,df2)
df= rbind(df,df3)
df = na.omit(df)
df$communityRegion <- ifelse(df$communityRegion == "SLC",1,0)
df1$communityRegion <- ifelse(df1$communityRegion == "SLC",1,0)
df2$communityRegion <- ifelse(df2$communityRegion == "SLC",1,0)
df3$communityRegion <- ifelse(df3$communityRegion == "SLC",1,0)
model1 = lm(pm25_1day~communityRegion+temperature+pressure+humidity+ communityRegion:temperature + temperature:pressure+ humidity:pressure + temperature:humidity + communityRegion:pressure + communityRegion:humidity,data = df)
summary(model1)
model2 = lm(pm25_1day~temperature + communityRegion:temperature + temperature:humidity, data = df)
summary(model2)
model3 = lm(pm25_1day~temperature:communityRegion + temperature:humidity, data=df)
summary(model3)
plot(model3)
model3 = lm(pm25_1day~temperature:communityRegion + temperature:humidity, humidity, data=df)
summary(model3)
model3 = lm(pm25_1day~temperature:communityRegion + temperature:humidity + humidity, data=df)
summary(model3)
model3 = lm(pm25_1day~temperature:communityRegion + temperature:humidity + humidity + temperature, data=df)
summary(model3)
model3 = lm(pm25_1day~temperature:communityRegion + temperature:humidity + humidity + temperature + pressure, data=df)
summary(model3)
model3 = lm(pm25_1day~ temperature:humidity + humidity + temperature + pressure, data=df)
summary(model3)
model1 = lm(pm25_1day~communityRegion+temperature+pressure+humidity+ communityRegion:temperature + temperature:pressure+ humidity:pressure + temperature:humidity + communityRegion:pressure + communityRegion:humidity,data = df)
summary(model1)
model3 = lm(pm25_1day~ humidity + temperature + pressure, data=df)
summary(model3)
temperature:humidity +
temperature:humidity +
model3 = lm(pm25_1day~ temperature:humidity + humidity + temperature + pressure, data=df)
summary(model3)
model3 = lm(pm25_1day~ temperature:humidity + humidity + temperature + pressure, data=df)
summary(model3)
model3 = lm(pm25_1day~ temperature:humidity + humidity + temperature + pressure + pressure:humidity, data=df)
summary(model3)
model3 = lm(pm25_1day~ temperature:humidity + humidity + temperature + pressure + pressure:humidity + temperature:pressure, data=df)
summary(model3)
model3 = lm(pm25_1day~ humidity, data=df)
summary(model3)
model3 = lm(pm25_1day~ humidity + pressure, data=df)
summary(model3)
model3 = lm(pm25_1day~ humidity + temperature, data=df)
summary(model3)
model3 = lm(pm25_1day~ temperature, data=df)
summary(model3)
model3 = lm(pm25_1day~ temperature + pressure + humidity, data=df)
summary(model3)
model3 = lm(pm25_1day~ pressure + humidity, data=df)
summary(model3)
model3 = lm(pm25_1day~ temperature:communityRegion + temperature:humidity, data=df)
summary(model3)
model3 = lm(pm25_1day~ pressure + humidity, data=df)
summary(model3)
#Naive Bayes
install.packages('e1071')
library('e1071')
#data
library('MASS')
head(Pima.te)
?Pima.te
attach(Pima.te)
#test.index = sample(c(1:dim(Pima.te)[1]),.2*dim(Pima.te)[1], replace = FALSE  )
#comma_vec <- paste(test.index, collapse = ", ")
test.index = c(136, 277, 23, 5, 193, 63, 172, 258, 151, 294, 296, 160, 85, 44, 16,
249, 317, 103, 100, 155, 229, 318, 200, 112, 81, 173, 110, 116, 83,
227, 58, 217, 61, 191, 208, 117, 148, 137, 169, 80, 234, 73, 254, 175, 206,
332, 182, 71, 53, 87, 141, 93, 190, 255, 134, 138, 327, 109, 194, 324, 25,
2, 76, 161, 8, 312)
nb.fit <- naiveBayes(type~. , data = Pima.te , subset = -test.index)
nb.fit
#predict values
nb.pred <- predict(nb.fit , Pima.te[test.index,])
table(nb.pred,type[test.index])#Hey this is pretty good! And with a transform!
mean(nb.pred==type[test.index]) #WOW :)
#reduced model
nb.fit2 <- naiveBayes(type~glu+bmi , data = Pima.te , subset = -test.index)
nb.fit2
#predict values
nb.pred <- predict(nb.fit2 , Pima.te[test.index,])
table(nb.pred,type[test.index])
mean(nb.pred==type[test.index])
?case2201
#data
library('MASS')
?case2201
library('e1071')
?case2201
library('Sleuth2')
library('Sleuth3')
?case2201
attach(case2201)
## EXPLORATION AND MODEL BUILDING
plot(Matings ~ Age,  log="y")
ageSquared  <- Age^2
myGlm1 <- glm(Matings ~ Age + ageSquared, family=poisson)
summary(myGlm1)  # No evidence of a need for ageSquared
## INFERENCE AND INTERPRETATION
myGlm2  <- update(myGlm1, ~ . - ageSquared)
summary(myGlm2)
beta  <- myGlm2$coef
exp(beta[2])
exp(confint(myGlm2,2))
cor(Pima.te[, -c(8)])
library("ISLR")
library()
library("ISLR2")
?Auto
attach(Auto)
mpg01 = ifelse(mpg > median(mpg), yes = 1, no = 0)
data <- data.fram(Auto, mpg01)
data <- data.frame(Auto, mpg01)
data
pairs(data)
print("Looking at which variables have fairly linear correlations to mpg as that will help us most to predict mpg01. Obviously mpg will help predict mpg01, also displacement, horsepower, and weight all seem like they will be helpful.")
library("ISLR2")
?Auto
attach(Auto)
mpg01 = ifelse(mpg > median(mpg), yes = 1, no = 0)
newAuto <- data.frame(Auto, mpg01)
pairs(data)
newAuto <- data.frame(Auto, mpg01)
pairs(data)
pairs(newAuto)
print("Looking at which variables have fairly linear correlations to mpg as that will help us most to predict mpg01. Obviously mpg will help predict mpg01, also displacement, horsepower, and weight all seem like they will be helpful.")
newAuto = data.fram(mpg01, apply(cbind(cylinders,weight, displacement, horsepower, acceleration), 2, scale), year)
newAuto = data.frame(mpg01, apply(cbind(cylinders,weight, displacement, horsepower, acceleration), 2, scale), year)
newAuto.head()
head(newAuto)
test.index = sample(c(1:dim(newAuto)[1]),.2*dim(newAuto)[1], replace=FALSE)
comma_vec <- paste(test.index, collapse = ", ")
comma_vec
#test.index = sample(c(1:dim(newAuto)[1]),.2*dim(newAuto)[1], replace=FALSE)
#comma_vec <- paste(test.index, collapse = ", ")
test.index =c(82, 53, 268, 356, 208, 51, 362, 96, 278, 22, 118, 170, 327, 290, 115, 111, 329, 67, 289, 3, 195, 333, 260, 154, 261, 106, 345, 274, 286, 376, 371, 378, 128, 25, 328, 158, 102, 165, 146, 77, 324, 148, 303, 315, 270, 56, 340, 85, 284, 369, 247, 254, 285, 155, 295, 388, 291, 60, 319, 300, 58, 255, 227, 305, 75, 127, 103, 137, 246, 140, 206, 218, 95, 172, 232, 320, 383, 221)
library(MASS)
newAuto = data.frame(mpg01, apply(cbind(newAuto), 2, scale), year)
#test.index = sample(c(1:dim(newAuto)[1]),.2*dim(newAuto)[1], replace=FALSE)
#comma_vec <- paste(test.index, collapse = ", ")
test.index =c(82, 53, 268, 356, 208, 51, 362, 96, 278, 22, 118, 170, 327, 290, 115, 111, 329, 67, 289, 3, 195, 333, 260, 154, 261, 106, 345, 274, 286, 376, 371, 378, 128, 25, 328, 158, 102, 165, 146, 77, 324, 148, 303, 315, 270, 56, 340, 85, 284, 369, 247, 254, 285, 155, 295, 388, 291, 60, 319, 300, 58, 255, 227, 305, 75, 127, 103, 137, 246, 140, 206, 218, 95, 172, 232, 320, 383, 221)
library(MASS)
lda.fit = lad(mpg01 ~ weight + displacement + horsepower, data = newAuto, subset = -train)
lda.fit = lda(mpg01 ~ weight + displacement + horsepower, data = newAuto, subset = -train)
lda.fit = lda(mpg01 ~ weight + displacement + horsepower, data = newAuto, subset = -test.index)
lda.pred = predict(lda.fit, test.index)
lda.pred = predict(lda.fit, newAuto[test.index])
lda.pred = predict(lda.fit, newAuto[test.index,])
mean(lda.pred$class != mpg01.test.index)
lda.pred
mean(lda.pred$class != test.index)
mean(lda.pred$class != mpg01[test.index])
table(lda.pred,type[test.index])
mean(lda.pred$class != mpg01.[test.index])
mean(lda.pred$class != mpg01[test.index])
lda.fit = lda(mpg01 ~ weight + displacement + horsepower + cylinders, data = newAuto, subset = -test.index)
lda.pred = predict(lda.fit, newAuto[test.index,])
mean(lda.pred$class != mpg01[test.index])
lda.fit = lda(mpg01 ~ weight + displacement + horsepower, data = newAuto, subset = -test.index)
lda.pred = predict(lda.fit, newAuto[test.index,])
mean(lda.pred$class != mpg01[-test.index])
lda
lda.pred
lda.fit
train.index = -test.index
mean(lda.pred$class != mpg01[train.index])
mean(lda.pred$class != -mpg01[test.index])
mean(lda.pred$class != mpg01[test.index])
qda.fit = qda(mpg01 ~ weight + displacement + horsepower, data = newAuto, subset = -test.index)
qda.pred = predict(qda.fit, newAuto[test.index,])
mean(qda.pred$class != mpg01[test.index])
glm.fit = glm(mpg01 ~ weight + displacement + horsepower, data = newAuto, subset = -test.index, family= binomial)
glm.fit = glm(mpg01 ~ weight + displacement + horsepower, data = newAuto, subset = -test.index, family= binomial)
glm.probs = predict(glm.fit, newAuto[test.index], type = "response")
glm.probs = predict(glm.fit, newAuto.test.index, type = "response")
glm.probs = predict(glm.fit, newAuto[test.index] type = "response")
newAuto[test.index]
glm.probs = predict(glm.fit, newAuto[test.index,] type = "response")
glm.probs = predict(glm.fit, newAuto[test.index,], type = "response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > .5] = 1
mean(glm.pred != mpg01.test)
mean(glm.pred != mpg01[test.index])
mean(glm.pred != -mpg01[test.index])
mean(glm.pred != mpg01[test.index])
mean(lda.pred$class != mpg01[test.index])
nb.fit = naiveBayes(mpg01 ~ weight + displacement + horsepower, data = newAuto, subset = -test.index)
nb.fit
nb.pred = predict(nb.fit, newAuto[test.index,])
table(nb.pred,mpg01[test.index])
mean(glm.pred == mpg01[test.index])
#test.index = sample(c(1:dim(ex2012)[1]),.2*dim(ex2012)[1], replace = FALSE  )
#comma_vec <- paste(test.index, collapse = ", ")
test.index = c(117, 45, 90, 40, 66, 41, 8, 25, 95, 20, 110, 84, 5, 74, 111, 55, 56, 79, 29, 2, 1, 60, 69, 58)
nb.fit <- naiveBayes(Group~CK+H , data = ex2012 , subset = -test.index)
nb.fit
#predict values
nb.pred <- predict(nb.fit , ex2012[test.index,])
table(nb.pred,Group[test.index])#Hey this is pretty good! And with a transform!
#data
library('Sleuth3')
head(ex2012)
?ex2012
attach(ex2012)
#test.index = sample(c(1:dim(ex2012)[1]),.2*dim(ex2012)[1], replace = FALSE  )
#comma_vec <- paste(test.index, collapse = ", ")
test.index = c(117, 45, 90, 40, 66, 41, 8, 25, 95, 20, 110, 84, 5, 74, 111, 55, 56, 79, 29, 2, 1, 60, 69, 58)
nb.fit <- naiveBayes(Group~CK+H , data = ex2012 , subset = -test.index)
nb.fit
#predict values
nb.pred <- predict(nb.fit , ex2012[test.index,])
table(nb.pred,Group[test.index])#Hey this is pretty good! And with a transform!
mean(nb.pred==Group[test.index]) #WOW :)
library("ISLR2")
?Auto
attach(Auto)
mpg01 = ifelse(mpg > median(mpg), yes = 1, no = 0)
newAuto <- data.frame(Auto, mpg01)
pairs(newAuto)
print("Looking at which variables have fairly linear correlations to mpg as that will help us most to predict mpg01. Obviously mpg will help predict mpg01, but we won't use it since that's cheating. Also displacement, horsepower, and weight all seem like they will be helpful.")
newAuto = data.frame(mpg01, apply(cbind(newAuto), 2, scale), year)
#test.index = sample(c(1:dim(newAuto)[1]),.2*dim(newAuto)[1], replace=FALSE)
#comma_vec <- paste(test.index, collapse = ", ")
test.index =c(82, 53, 268, 356, 208, 51, 362, 96, 278, 22, 118, 170, 327, 290, 115, 111, 329, 67, 289, 3, 195, 333, 260, 154, 261, 106, 345, 274, 286, 376, 371, 378, 128, 25, 328, 158, 102, 165, 146, 77, 324, 148, 303, 315, 270, 56, 340, 85, 284, 369, 247, 254, 285, 155, 295, 388, 291, 60, 319, 300, 58, 255, 227, 305, 75, 127, 103, 137, 246, 140, 206, 218, 95, 172, 232, 320, 383, 221)
library(MASS)
lda.fit = lda(mpg01 ~ weight + displacement + horsepower, data = newAuto, subset = -test.index)
lda.pred = predict(lda.fit, newAuto[test.index,])
mean(lda.pred$class != mpg01[test.index])
print("Looks like I got a test error of around 10.26%")
qda.fit = qda(mpg01 ~ weight + displacement + horsepower, data = newAuto, subset = -test.index)
qda.pred = predict(qda.fit, newAuto[test.index,])
mean(qda.pred$class != mpg01[test.index])
print("Looks like I got a test error of about 7.69%")
glm.fit = glm(mpg01 ~ weight + displacement + horsepower, data = newAuto, subset = -test.index, family= binomial)
glm.probs = predict(glm.fit, newAuto[test.index,], type = "response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > .5] = 1
mean(glm.pred == mpg01[test.index])
print("Looks like I got a test error of about 8.97%")
nb.fit = naiveBayes(mpg01 ~ weight + displacement + horsepower, data = newAuto, subset = -test.index)
nb.pred = predict(nb.fit, newAuto[test.index,])
table(nb.pred,mpg01[test.index])
mean(nb.pred != mpg01[test.index])
library("class")
train.x = newAuto[-test.index,]
test.x = newAuto[test.index,]
train.y = mpg01[-test.index]
test.y = mpg01[test.index]
set.seed(42069)
knn.pred = knn(train.x, test.x, train.y,k=2)
knn.pred = knn(train.x, test.x, train.y,k=2)
newAuto[1][mpg]
newAuto[mpg][1]
newAuto[1,"mpg"]
train.x = newAuto[-test.index,"weight","displacement"]
newAuto[1,mpg + displacement]
newAuto[1,mpg]
newAuto[1,"mpg]
newAuto[1,"mpg"]
newAuto[1,"mpg, displacement"]
newAuto[1,c("mpg", "displacement")]
newAuto = newAuto[:,c("weight","displacement", "horsepower")]
newAuto = newAuto[,c("weight","displacement", "horsepower")]
train.x = newAuto[-test.index,]
test.x = newAuto[test.index,]
train.y = mpg01[-test.index]
test.y = mpg01[test.index]
set.seed(42069)
knn.pred = knn(train.x, test.x, train.y,k=2)
knn.pred
mean(knn.pred != mpg01[test.index])
knn.pred = knn(train.x, test.x, train.y,k=2)
mean(knn.pred != mpg01[test.index])
knn.pred = knn(train.x, test.x, train.y,k=3)
mean(knn.pred != mpg01[test.index])
knn.pred = knn(train.x, test.x, train.y,k=1)
mean(knn.pred != mpg01[test.index])
knn.pred = knn(train.x, test.x, train.y,k=5)
mean(knn.pred != mpg01[test.index])
for (row in 1:nrow(newAuto)) {
knn.pred = knn(train.x, test.x, train.y,k=row)
mean(knn.pred != mpg01[test.index])
}
warnings()
for (row in 1:314)) {
knn.pred = knn(train.x, test.x, train.y,k=row)
mean(knn.pred != mpg01[test.index])
}
for (row in 1:314)) {
knn.pred = knn(train.x, test.x, train.y,k=row)
mean(knn.pred != mpg01[test.index])
}
for (row in 1:314) {
knn.pred = knn(train.x, test.x, train.y,k=row)
mean(knn.pred != mpg01[test.index])
}
Klist = ()
Klist = c()
for (row in 1:314) {
knn.pred = knn(train.x, test.x, train.y,k=row)
errorList = append(mean(knn.pred != mpg01[test.index]))
Klist = append(row)
}
for (row in 1:314) {
knn.pred = knn(train.x, test.x, train.y,k=row)
errorList = c(errorList,mean(knn.pred != mpg01[test.index]))
Klist = c(Klist, row)
}
errorList = c()
for (row in 1:314) {
knn.pred = knn(train.x, test.x, train.y,k=row)
errorList = c(errorList,mean(knn.pred != mpg01[test.index]))
Klist = c(Klist, row)
}
print(max(errorList))
print(min(errorList))
which.min(errorList)
knn.pred = knn(train.x, test.x, train.y,k=1)
mean(knn.pred != mpg01[test.index])
knn.pred = knn(train.x, test.x, train.y,k=4)
mean(knn.pred != mpg01[test.index])
plot(Klist,errorList)
which.min(errorList)
plot(Klist, errorList)
install.packages("boot")
upgrade
update.packages()
update.packages()
library("ISLR")
library('ISLR')
install.packages('ISLR')
library('ISLR')
head(Default)
attach(Default)
set.seed(420)
model1 <- glm(default ~ income + balance, family = "binomial")
summary(model1)
detach(Default)
set.seed(420)
model1 <- glm(default ~ income + balance, family = "binomial", data=Default)
summary(model1)
boot.fn <- function(data, index) {
model <- glm(default ~ income + balance, family = "binomial", data = data, subset = index)
return(coef(model))
}
boot.fn
boot.fn()
boot.fn(Default,[1:2])
boot.fn(Default,c(1,2,3))
model
model1
summary(model1)
coef(summary(model1))[2:3,2]
coef(summary(model1))
one_boot <- function(data) {
index <- sample(1:nrow(data), nrow(data), replace=T)
model <- glm(default ~ income + balance, data = data, family = "binomial", subset = index)
return(ceof(summary(model))[2:3,2])
}
one_bott(Default)
one_boot(Default)
one_boot <- function(data) {
index <- sample(1:nrow(data), nrow(data), replace=T)
model <- glm(default ~ income + balance, data = data, family = "binomial", subset = index)
return(coef(summary(model))[2:3,2]) #(income, balance)
#This returns a single bootstraped estimate of standard error
}
one_boot(Default)
one_boot <- function(data) {
index <- sample(1:nrow(data), nrow(data), replace=T)
model <- glm(default ~ income + balance, data = data, family = "binomial", subset = index)
return(coef(summary(model))[2:3,2]) #(income, balance)
#This returns a single bootstraped estimate of standard error
}
one_boot(Default)
one_boot(Default)
one_boot(Default)
one_boot(Default)
one_boot(Default)
one_boot(Default)
one_boot <- function(data) {
index <- sample(1:nrow(data), nrow(data), replace=T)
model <- glm(default ~ income + balance, data = data[index,], family = "binomial")
return(coef(summary(model))[2:3,2]) #(income, balance)
#This returns a single bootstraped estimate of standard error
}
one_boot(Default)
one_boot(Default)
one_boot(Default)
all_the_boots = c()
num.boots = 100
for (i in num.boots) {
all_the_boots = c(all_the_boots, one_boot(Default))
}
all_the_boots = c()
num.boots = 100
for (i in num.boots) {
all_the_boots = rbind(all_the_boots, one_boot(Default))
}
all_the_boots
all_the_boots = c()
num.boots = 100
for (i in 1:num.boots) {
all_the_boots = rbind(all_the_boots, one_boot(Default))
}
all_the_boots
all_the_boots = c()
num.boots = 50
for (i in 1:num.boots) {
all_the_boots = rbind(all_the_boots, one_boot(Default))
}
all_the_boots
names(all_the_boots) = c("income","balance")
head(all_the_boots)
head(all_the_boots)
mean(all_the_boots)
mean(all_the_boots[1])
mean(all_the_boots[2])
mean(all_the_boots[,1])
mean(all_the_boots[,2])
BS_estimate <- function(num.boots) {
all_the_boots = c()
for (i in 1:num.boots) {
all_the_boots = rbind(all_the_boots, one_boot(Default))
}
names(all_the_boots) = c("income","balance")
head(all_the_boots)
mean(all_the_boots[,1])
mean(all_the_boots[,2])
}
BS_estimate <- function(num.boots) {
all_the_boots = c()
for (i in 1:num.boots) {
all_the_boots = rbind(all_the_boots, one_boot(Default))
}
names(all_the_boots) = c("income","balance")
head(all_the_boots)
return(mean(all_the_boots[,1]),mean(all_the_boots[,2]))
}
for(i in 1:100) {
BS_estimate(i)
}
BS_estimate <- function(num.boots) {
all_the_boots = c()
for (i in 1:num.boots) {
all_the_boots = rbind(all_the_boots, one_boot(Default))
}
names(all_the_boots) = c("income","balance")
head(all_the_boots)
return(c(mean(all_the_boots[,1]),mean(all_the_boots[,2])))
}
for(i in 1:100) {
BS_estimate(i)
}
for(i in 1:100) {
print(BS_estimate(i))
}
train <- read.csv("trainCSV.csv")
setwd("~/Documents/GitHub/Project2")
train <- read.csv("trainCSV.csv")
train <- read.csv("train.csv")
test = read.csv("test.csv")
train <- read.csv("training.csv")
test = read.csv("testest.csv")
test = read.csv("testing.csv")
library('ISLR')
set.seed(420)
model1 <- glm(default ~ income + balance, family = "binomial", data=Default)
summary(model1)
print("The standard error for income is 4.985e-06 and for balance the standard error is 2.274e-04")
boot.fn <- function(data, index) {
model <- glm(default ~ income + balance, family = "binomial", data = data, subset = index)
return(coef(model))
}
one_boot <- function(data) {
index <- sample(1:nrow(data), nrow(data), replace=T)
model <- glm(default ~ income + balance, data = data[index,], family = "binomial")
return(coef(summary(model))[2:3,2]) #(income, balance)
#This returns a single bootstraped estimate of standard error
}
one_boot(Default)
BS_estimate <- function(num.boots) {
all_the_boots = c()
for (i in 1:num.boots) {
all_the_boots = rbind(all_the_boots, one_boot(Default))
}
names(all_the_boots) = c("income","balance")
head(all_the_boots)
return(c(mean(all_the_boots[,1]),mean(all_the_boots[,2])))
}
BS_estimate(100)
